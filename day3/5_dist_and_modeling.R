Distributions and Modeling
---------------------------
Using summary statistics and plots to understand data is great, but they have their
limitations. Statistics don’t give you the shape of data, and plots aren’t scalable
to many variables (with more than five or six, things start to get confusing),
nor are they scalable in number (since you have to physically look at each one). 
Neither statistics nor plots are very good at giving you predictions from the data.
This is where models come in: if you understand enough about the structure of the 
data to be able to run a suitable model, then you can pass quantitative judgments 
about the data and make predictions.

Random Numbers
Random numbers are critical to many analyses, and consequently R has a wide variety 
of functions for sampling from different distributions.

The sample Function
We’ve seen the sample function a few times already. It’s an important workhorse 
function for generating random values, and its behavior has a few quirks, so it’s 
worth getting to know it more thoroughly. If you just pass it a number, n, it 
will return a permutation of the natural numbers from 1 to n:
  sample(7)
## [1] 1 2 5 7 4 6 3

If you give it a second value, it will return that many random numbers between 1 and n:
  sample(7, 5)
## [1] 7 2 3 1 5

Notice that all those random numbers are different. By default, sample samples 
without replacement. That is, each value can only appear once. To allow sampling 
with replacement, pass replace = TRUE:
  sample(7, 10, replace = TRUE)
##  [1] 4 6 1 7 5 3 6 7 4 2

sample(colors(), 5) #a great way to pick the color scheme for your house
## [1] "grey53"       "deepskyblue2" "gray94"       "maroon2"
## [5] "gray18"

If we were feeling more adventurous, we could pass it some dates:
  sample(.leap.seconds, 4)
## [1] "2012-07-01 01:00:00 BST" "1994-07-01 01:00:00 BST"
## [3] "1981-07-01 01:00:00 BST" "1990-01-01 00:00:00 GMT"

We can also weight the probability of each input value being returned by passing a 
prob argument. In the next example, we use R to randomly decide which month to go 
on holiday, then fudge it to increase our chances of a summer break:
  weights <- c(1, 1, 2, 3, 5, 8, 13, 21, 8, 3, 1, 1)
sample(month.abb, 1, prob = weights)
## [1] "Jul"

Sampling from Distributions
Oftentimes, we want to generate random numbers from a probability distribution. 
R has functions available for sampling from almost one hundred distributions, 
mixtures, and copulas across the various packages. 
Most of the random number generation functions have the name r<distn>. For 
example, we’ve already seen runif, which generates uniform random numbers, and 
rnorm, which generates normally distributed random numbers. The first parameter 
for each of these functions is the number of random numbers to generate, and 
further parameters affect the shape of the distribution. For example, runif allows 
you to set the lower and upper bounds of the distribution:
  runif(5)         #5 uniform random numbers between 0 and 1
runif(5, 1,10)   #5 uniform random numbers between 1 and 10
rnorm(5)         #5 normal random numbers with mean 0 and std dev 1
rnorm(5, 3, 7)   #5 normal random numbers with mean 3 and std dev 7

Random numbers generated by R are, like with any other software, actually 
pseudorandom. That is, they are generated by an algorithm rather than a genuinely 
random process. R supports several algorithms out of the box, as described on 
the ?RNG page, and more specialist algorithms (for parallel number generation, 
                                               for example) in other packages. 
You can see which algorithms are used for uniform and normal 
random number generation with the RNGkind function (sampling from other distributions 
                                                    typically uses some function 
                                                    of uniform or normal random numbers):
  RNGkind()
## [1] "Mersenne-Twister" "Inversion"
Random number generators require a starting point to generate numbers from, 
known as a “seed.” By setting the seed to a particular value, you can guarantee 
that the same random numbers will be generated each time you run the same piece of 
code. For example, this book fixes the seed so that the examples are the same each 
time the book is built. You can set the seed with the set.seed function. It takes a 
positive integer as an input. Which integer you choose doesn’t matter; different 
seeds just give different random numbers:
  set.seed(1)
runif(5)

Models
------
In the gonorrhoea dataset:
  head(gonorrhoea)
  Rate ~ Year + Age.Group + Ethnicity + Gender
Here, Rate is the response, and we have chosen to include four independent 
variables (year/age group/ethnicity/gender). For models that can include an 
intercept (that’s more or less all regression models), a formula like this will 
implicitly include that intercept. If we passed it into a linear regression model, 
it would mean:

Rate = alpha_0 + alpha_1 * Year + alpha_2 * Age.Group + alpha_3 * Ethnicity + 
  alpha_4 * Gender + epsilon

where each alpha_i is a constant to be determined by the model, and epsilon is a 
normally distributed error term.
If we didn’t want the intercept term (alpha_0), we could add a zero to the righthand 
side to suppress it:
  Rate ~ 0 + Year + Age.Group + Ethnicity + Gender

Rate = alpha_1 * Year + alpha_2 * Age.Group + alpha_3 * Ethnicity + 
  alpha_4 * Gender + epsilon

This does not include interaction.
There are two ways of putting the level of interaction. First, you can add individual 
interactions using colons. In the following example, Year:Ethnicity is a two-way 
interaction between those terms, and Year:Ethnicity:Gender is a three-way interaction:
  Rate ~ Year + Ethnicity + Gender + Year:Ethnicity + Year:Ethnicity:Gender

This fine-grained approach can become cumbersome to type if you have more than three 
or four variables, though, so an alternate syntax lets you include all interactions up 
to a certain level using carets (^). The next example includes the year, ethnicity, 
and gender, and the three two-way interactions:
  Rate ~ (Year + Ethnicity + Gender) ^ 2

You may have spotted a problem with including var ^ 2. That syntax is reserved for 
interactions, so if you want to include powers of things, wrap them in I():
  Rate ~ I(Year ^ 2)  #year squared, not an interaction

A First Model: Linear Regressions
Ordinary least squares linear regressions are the simplest in an extensive family 
of regression models. The function for calculating them is concisely if not clearly 
named: lm, short for “linear model.” It accepts a formula of the type we’ve just 
discussed, and a data frame that contains the variables to model. Let’s take a look at 
the gonorrhoea dataset. For simplicity, we’ll ignore interactions:
  model1 <- lm(Rate ~ Year + Age.Group + Ethnicity + Gender, gonorrhoea)

model1
If we print the model variable, it lists the coefficients for each input variable 
alpha_i, (the  values). If you look closely, you’ll notice that for both of the categorical 
variables that we put into the model (age group and ethnicity), one category has no 
coefficient. For example, the 0 to 4 age group is missing, and the American Indians 
& Alaskan Natives ethnicity is also missing.

These “missing” categories are included in the intercept. In the following output, 
the intercept value of 5,540 people infected with gonorrhoea per 100,000 people 
applies to 0- to 4-year-old female American Indians and Alaskan Natives in the year 0. 
To predict infection rates in the years up to 2013, we would add 2013 times the 
coefficient for Year, -2.77. To predict the effect on 25- to 29-year-olds of the same 
ethnicity, we would add the coefficient for that age group, 291.098:
  
  The “0- to 4-year-old female American Indians and Alaskan Natives” group was 
chosen because it consists of the first level of each of the factor variables. 
We can see those factor levels by looping over the dataset and calling levels:
  lapply(Filter(is.factor, gonorrhoea), levels)
## $Age.Group
##  [1] "0 to 4"     "5 to 9"     "10 to 14"   "15 to 19"   "20 to 24"
##  [6] "25 to 29"   "30 to 34"   "35 to 39"   "40 to 44"   "45 to 54"
## [11] "55 to 64"   "65 or more"
##
## $Ethnicity
## [1] "American Indians & Alaskan Natives"
## [2] "Asians & Pacific Islanders"
## [3] "Hispanics"
## [4] "Non-Hispanic Blacks"
## [5] "Non-Hispanic Whites"
##
## $Gender
## [1] "Female" "Male"

As well as knowing the size of the effect of each input variable, we usually 
want to know which variables were significant. The summary function is overloaded 
to work with lm to do just that. The most exciting bit of the output from summary 
is the coefficients table. The Estimate column shows the coefficients that we’ve 
seen already, and the fourth column, Pr(>|t|), shows the p-values. The fifth 
column gives a star rating: where the p-value is less than 0.05 a variable gets 
one star, less than 0.01 is two stars, and so on. This makes it easy to quickly 
see which variables had a significant effect:
  summary(model1)
##
## Call:
## lm(formula = Rate ~ Year + Age.Group + Ethnicity + Gender, data = gonorrhoea)
##
## Residuals:
##    Min     1Q Median     3Q    Max
## -376.7 -130.6   37.1   90.7 1467.1
##
## Coefficients:
##                                      Estimate Std. Error t value Pr(>|t|)
## (Intercept)                          5540.496  14866.406    0.37   0.7095
## Year                                   -2.770      7.400   -0.37   0.7083
## Age.Group5 to 9                        -0.614     51.268   -0.01   0.9904
## Age.Group10 to 14                      15.268     51.268    0.30   0.7660
## Age.Group15 to 19                     415.698     51.268    8.11  3.0e-15
## Age.Group20 to 24                     546.820     51.268   10.67  < 2e-16
## Age.Group25 to 29                     291.098     51.268    5.68  2.2e-08
## Age.Group30 to 34                     155.872     51.268    3.04   0.0025
## Age.Group35 to 39                      84.612     51.268    1.65   0.0994
## Age.Group40 to 44                      49.506     51.268    0.97   0.3346
## Age.Group45 to 54                      27.364     51.268    0.53   0.5937
## Age.Group55 to 64                       8.684     51.268    0.17   0.8656
## Age.Group65 or more                     1.178     51.268    0.02   0.9817
## EthnicityAsians & Pacific Islanders   -82.923     33.093   -2.51   0.0125
## EthnicityHispanics                    -49.000     33.093   -1.48   0.1392
## EthnicityNon-Hispanic Blacks          376.204     33.093   11.37  < 2e-16
## EthnicityNon-Hispanic Whites          -68.263     33.093   -2.06   0.0396
## GenderMale                            -17.892     20.930   -0.85   0.3930
##
## (Intercept)
## Year
## Age.Group5 to 9
## Age.Group10 to 14
## Age.Group15 to 19                   ***
## Age.Group20 to 24                   ***
## Age.Group25 to 29                   ***
## Age.Group30 to 34                   **
## Age.Group35 to 39                   .
## Age.Group40 to 44
## Age.Group45 to 54
## Age.Group55 to 64
## Age.Group65 or more
## EthnicityAsians & Pacific Islanders *
## EthnicityHispanics
## EthnicityNon-Hispanic Blacks        ***
## EthnicityNon-Hispanic Whites        *
## GenderMale
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## Residual standard error: 256 on 582 degrees of freedom
## Multiple R-squared:  0.491,  Adjusted R-squared:  0.476
## F-statistic: 33.1 on 17 and 582 DF,  p-value: <2e-16

What is bias?
  
  Bias is the difference between the average prediction of our model and the 
correct value which we are trying to predict. Model with high bias pays very 
little attention to the training data and oversimplifies the model. It always 
leads to high error on training and test data.

What is variance?
  
  Variance is the variability of model prediction for a given data point or 
a value which tells us spread of our data. Model with high variance pays a lot 
of attention to training data and does not generalize on the data which it 
hasn’t seen before. As a result, such models perform very well on training data but 
has high error rates on test data.

In supervised learning, underfitting happens when a model unable to capture 
the underlying pattern of the data. These models usually have high bias and 
low variance. It happens when we have very less amount of data to build an 
accurate model or when we try to build a linear model with a nonlinear data. 
Also, these kind of models are very simple to capture the complex patterns in 
data like Linear and logistic regression.

In supervised learning, overfitting happens when our model captures the noise 
along with the underlying pattern in data. It happens when we train our model a 
lot over noisy dataset. These models have low bias and high variance. These 
models are very complex like Decision trees which are prone to overfitting.

t-statistic:
------------
  Simple linear regression lives up to its name: it is a very straightforward
approach for predicting a quantitative response Y on the basis of a sin-
  gle predictor variable X. It assumes that there is approximately a linear
relationship between X and Y

  Y ≈ β_0 + β_1 X
  In practice, β_0 and β_1 are unknown. So before we can use to make
predictions, we must use data to estimate the coefficients. Let
(x_1 , y_1 ), (x_2 , y_2 ), . . . , (x_n , y_n )
represent n observation pairs, each of which consists of a measurement
of X and a measurement of Y . In the Advertising example, this data
set consists of the TV advertising budget and product sales in n = 200
different markets. Our goal is to obtain coefficient estimates β̂ 0 and β̂ 1 such
that the linear model fits the available data well—that is, so that 
y_i ≈ β̂_ 0 + β̂_ 1 x i for i =1, . . . , n
. In other words, we want to find an intercept β̂ 0 and a slope β̂ 1 such
that the resulting line is as close as possible to the n = 200 data points.
There are a number of ways of measuring closeness. However, by far the
most common approach involves minimizing the least squares criterion.

Let ŷ i = β̂ 0 + β̂ 1 x i be the prediction for Y based on the ith value of X.
Then e i = y i − ŷ i represents the ith residual—this is the difference between
the ith observed response value and the ith response value that is predicted
by our linear model. We define the residual sum of squares (RSS) as
RSS = e1^2 + e2^2 + ··· + e2^n ,
or equivalently as
RSS = (y1 − β̂ 0 − β̂ 1 x 1 ) 2 + y 2 − β̂ 0 − β̂ 1 x 2 ) 2 + . . .+(y n − β̂ 0 − β̂ 1 x n ) 

The least squares approach chooses β̂ 0 and β̂ 1 to minimize the RSS.

Look at pg 66 of ISLR book for definition of standard error (SE).

Standard errors can also be used to perform hypothesis tests on the
coefficients. The most common hypothesis test involves testing the null
hypothesis of
H0 : There is no relationship between X and Y versus the alternative hypothesis
Ha : There is some relationship between X and Y.

Mathematically, this corresponds to testing
H0 : β1 = 0
versus
Ha : β1 != 0,
since if β1 = 0 then the model reduces to Y = β 0 + epsilon, and X is
not associated with Y . To test the null hypothesis, we need to determine
whether β̂ 1 , our estimate for β 1 , is sufficiently far from zero that we can
be confident that β 1 is non-zero. How far is far enough? This of course
depends on the accuracy of β̂ 1 —that is, it depends on SE( β̂ 1 ). If SE( β̂ 1 ) is
small, then even relatively small values of β̂ 1 may provide strong evidence

In practice, we compute a t-statistic, given by
t= (β̂ 1 − 0)/SE( β̂ 1 )
which measures the number of standard deviations that β̂ 1 is away from
0. If there really is no relationship between X and Y , then we expect
that above equation will have a t-distribution with n − 2 degrees of freedom. 
The t- distribution has a bell shape and for values of n greater than approximately
30 it is quite similar to the normal distribution. Consequently, it is a simple
matter to compute the probability of observing any number equal to |t| or
larger in absolute value, assuming β 1 = 0. We call this probability the p-value.
Roughly speaking, we interpret the p-value as follows: a small p-value indicates
that it is unlikely to observe such a substant ial association between the pre-
  dictor and the response due to chance, in the absence of any real association
between the predictor and the response. Hence, if we see a small p-value,then we can 
infer that there is an association between the predictor and the
response.

p-value or p-statistic:
----------------------
The P value, or calculated probability, is the probability of finding the observed, 
or more extreme, results when the null hypothesis (H0) of a study question is true – 
the definition of ‘extreme’ depends on how the hypothesis is being tested. 
P is also described in terms of rejecting H0 when it is actually true, however, it 
is not a direct probability of this state.

The null hypothesis is usually an hypothesis of "no difference" e.g. no difference 
between blood pressures in group A and group B. Define a null hypothesis for each 
study question clearly before the start of your study.

The only situation in which you should use a one sided P value is when a large 
change in an unexpected direction would have absolutely no relevance to your 
study. This situation is unusual; if you are in any doubt then use a two sided P value.

The term significance level (alpha) is used to refer to a pre-chosen probability 
and the term "P value" is used to indicate a probability that you calculate after
a given study.

The alternative hypothesis (H1) is the opposite of the null hypothesis; in plain 
language terms this is usually the hypothesis you set out to investigate. For example, 
question is "is there a significant (not due to chance) difference in blood 
pressures between groups A and B if we give group A the test drug and group B a 
sugar pill?" and alternative hypothesis is " there is a difference in blood pressures 
between groups A and B if we give group A the test drug and group B a sugar pill".

If your P value is less than the chosen significance level then you reject the null 
hypothesis i.e. accept that your sample gives reasonable evidence to support the 
alternative hypothesis. It does NOT imply a "meaningful" or "important" difference; 
that is for you to decide when considering the real-world relevance of your result.

The choice of significance level at which you reject H0 is arbitrary. Conventionally 
the 5% (less than 1 in 20 chance of being wrong), 1% and 0.1% (P < 0.05, 0.01 and 0.001) 
levels have been used. These numbers can give a false sense of security.

In the ideal world, we would be able to define a "perfectly" random sample, 
the most appropriate test and one definitive conclusion. We simply cannot. 
What we can do is try to optimise all stages of our research to minimise sources 
of uncertainty. When presenting P values some groups find it helpful to use the 
asterisk rating system as well as quoting the P value:
  P < 0.05 *
  P < 0.01 **
  P < 0.001

Most authors refer to statistically significant as P < 0.05 and statistically highly 
significant as P < 0.001 (less than one in a thousand chance of being wrong).

The asterisk system avoids the woolly term "significant". Please note, however, 
that many statisticians do not like the asterisk rating system when it is used without 
showing P values. As a rule of thumb, if you can quote an exact P value then do. 
You might also want to refer to a quoted exact P value as an asterisk in text 
narrative or tables of contrasts elsewhere in a report.

At this point, a word about error. Type I error is the false rejection of the 
null hypothesis and type II error is the false acceptance of the null hypothesis. 

r-squared:
  What Is Goodness-of-Fit for a Linear Model?
  Residual = Observed value - Fitted value
Linear regression calculates an equation that minimizes the distance between 
the fitted line and all of the data points. Technically, ordinary least squares 
(OLS) regression minimizes the sum of the squared residuals.

In general, a model fits the data well if the differences between the observed 
values and the predicted values are small and unbiased

R-squared is a statistical measure of how close the data are to the fitted 
regression line. It is also known as the coefficient of determination, or the 
coefficient of multiple determination for multiple regression.

The definition of R-squared is fairly straight-forward; it is the percentage of 
the response variable variation that is explained by a linear model. Or:
  
R-squared = Explained variation / Total variation
R^2 = (TSS − RSS)/TSS
=1 −RSS/TSS
R-squared is always between 0 and 100%:
  
0% indicates that the model explains none of the variability of the response data 
around its mean.
100% indicates that the model explains all the variability of the response data 
around its mean.
In general, the higher the R-squared, the better the model fits your data. 

adjusted R-squared:
-------------------
  R-squared tends to reward you for including too many independent variables 
in a regression model, and it doesn’t provide any incentive to stop adding more. 
Adjusted R-squared and predicted R-squared use different approaches to help you 
fight that impulse to add too many. The protection that adjusted R-squared and 
predicted R-squared provide is critical because too many terms in a model can 
produce results that you can’t trust. These statistics help you include the correct 
number of independent variables in your regression model.

Some Problems with R-squared
Firstly, you cannot use R-squared to conclude whether 
your model is biased. To check for this bias, you need to check your residual plots. 
Unfortunately, there are yet more problems with R-squared that we need to address.

Problem 1: R-squared increases every time you add an independent variable to the 
model. The R-squared never decreases, not even when it’s just a chance correlation 
between variables. A regression model that contains more independent variables than 
another model can look like it provides a better fit merely because it contains 
more variables.

Problem 2: When a model contains an excessive number of independent variables and 
polynomial terms, it becomes overly customized to fit the peculiarities and random 
noise in your sample rather than reflecting the entire population. Statisticians 
call this overfitting the model, and it produces deceptively high R-squared values 
and a decreased capability for precise predictions.

Fortunately for us, adjusted R-squared and predicted R-squared address both of 
these problems.

What Is the Adjusted R-squared?
  Use adjusted R-squared to compare the goodness-of-fit for regression models that 
contain differing numbers of independent variables.

Let’s say you are comparing a model with five independent variables to a model with 
one variable and the five variable model has a higher R-squared. Is the model with five 
variables actually a better model, or does it just have more variables? To determine 
this, just compare the adjusted R-squared values!
  
  The adjusted R-squared adjusts for the number of terms in the model. Importantly, 
its value increases only when the new term improves the model fit more than expected 
by chance alone. The adjusted R-squared value actually decreases when the term doesn’t 
improve the model fit by a sufficient amount.

The example below shows how the adjusted R-squared increases up to a point and then 
decreases. On the other hand, R-squared blithely increases with each and every 
additional independent variable.


F-statistic:
  y = β_0 + β_1 X_1 + β_2 X_2 + · · · + β_p X_p + epsilon
  y^ = β_0^ + β_1^ X_1 + β_2^ X_2 + · · · + β_p^ X_p + epsilon
  RSS = sum(sqrt(y - y^)^2)
  Recall that in the simple linear regression setting, in order to determine
whether there is a relationship between the response and the predictor we
can simply check whether β_1 = 0. In the multiple regression setting with p
predictors, we need to ask whether all of the regression coefficients are zero,
i.e. whether β 1 = β 2 = · · · = β p = 0. As in the simple linear regression
setting, we use a hypothesis test to answer this question. We test the null
hypothesis,
H_0 : β_1 = β_2 = · · · = β_p = 0
versus the alternative
H_a : at least one β_j is non-zero.
This hypothesis test is performed by computing the F-statistic,
F =(TSS − RSS)/p
-----------------
RSS/(n − p − 1)
TSS = (y_i − ȳ)^2 is the total sum of squares

Residual Standard Error:
  The quality of a linear regression fit is typically assessed
using two related quantities: the residual standard error (RSE) and the R^2
statistic.
Due to the presence of these error terms, even if we knew the
true regression line (i.e. even if β 0 and β 1 were known), we would not be
able to perfectly predict Y from X. The RSE is an estimate of the standard
deviation of epsilon. Roughly speaking, it is the average amount that the response
will deviate from the true regression line. It is computed using the formula

RSE = sqrt(1/(n-2)*RSS)
where RSS = sum(yi − ŷi )^2 .

Comparing and Updating Models
Rather than just accepting the first model that we think of, we often want to 
find a “best” model, or a small set of models that provide some insight.
This section demonstrates some metrics for measuring the quality of a model, 
such as p-values and log-likelihood measures. By using these metrics to compare 
models, you can automatically keep updating your model until you

Unfortunately, automatic updating of models like this (“stepwise regression”) 
is a poor method for choosing a model, and it gets worse as you increase the number 
of input variables.
Better methods for model selection, like model training or model averaging, are 
beyond the scope of this course. The CrossValidated statistics Q&A site has a 
good list of possibilities.
Tip
It is often a mistake to try to find a single “best” model. A good model is one that 
gives you insight into your problem — there may be several that do this.
In order to sensibly choose a model for our dataset, we need to understand a little 
about the things that affect gonorrhoea infection rates. Gonorrhoea is primarily 
sexually transmitted (with some transmission from mothers to babies during 
                      childbirth), so the big drivers are related to sexual culture: 
  how much unprotected sex people have, and with how many partners.
The p-value for Year is 0.71, meaning not even close to significant. Over such a 
short time series (five years of data), I’d be surprised if there were any changes in 
sexual culture big enough to have an important effect on infection rates, so let’s 
see what happens when we remove it.
Rather than having to completely respecify the model, we can update it using the 
update function. This accepts a model and a formula. We are just updating the 
righthand side of the formula, so the lefthand side stays blank. In the next 
example, . means “the terms that were already in the formula,” and – (minus) means 
“remove this next term”:
  model2 <- update(model1, ~ . - Year)
summary(model2)
##
## Call:
## lm(formula = Rate ~ Age.Group + Ethnicity + Gender, data = gonorrhoea)
##
## Residuals:
##    Min     1Q Median     3Q    Max
## -377.6 -128.4   34.6   92.2 1472.6
##
## Coefficients:
##                                     Estimate Std. Error t value Pr(>|t|)
## (Intercept)                          -25.103     43.116   -0.58   0.5606
## Age.Group5 to 9                       -0.614     51.230   -0.01   0.9904
## Age.Group10 to 14                     15.268     51.230    0.30   0.7658
## Age.Group15 to 19                    415.698     51.230    8.11  2.9e-15
## Age.Group20 to 24                    546.820     51.230   10.67  < 2e-16
## Age.Group25 to 29                    291.098     51.230    5.68  2.1e-08
## Age.Group30 to 34                    155.872     51.230    3.04   0.0025
## Age.Group35 to 39                     84.612     51.230    1.65   0.0992
## Age.Group40 to 44                     49.506     51.230    0.97   0.3343
## Age.Group45 to 54                     27.364     51.230    0.53   0.5934
## Age.Group55 to 64                      8.684     51.230    0.17   0.8655
## Age.Group65 or more                    1.178     51.230    0.02   0.9817
## EthnicityAsians & Pacific Islanders  -82.923     33.069   -2.51   0.0124
## EthnicityHispanics                   -49.000     33.069   -1.48   0.1389
## EthnicityNon-Hispanic Blacks         376.204     33.069   11.38  < 2e-16
## EthnicityNon-Hispanic Whites         -68.263     33.069   -2.06   0.0394
## GenderMale                           -17.892     20.915   -0.86   0.3926
##
## (Intercept)
## Age.Group5 to 9
## Age.Group10 to 14
## Age.Group15 to 19                   ***
## Age.Group20 to 24                   ***
## Age.Group25 to 29                   ***
## Age.Group30 to 34                   **
## Age.Group35 to 39                   .
## Age.Group40 to 44
## Age.Group45 to 54
## Age.Group55 to 64
## Age.Group65 or more
## EthnicityAsians & Pacific Islanders *
## EthnicityHispanics
## EthnicityNon-Hispanic Blacks        ***
## EthnicityNon-Hispanic Whites        *
## GenderMale
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## Residual standard error: 256 on 583 degrees of freedom
## Multiple R-squared:  0.491,  Adjusted R-squared:  0.477
## F-statistic: 35.2 on 16 and 583 DF,  p-value: <2e-16

The anova function computes ANalysis Of VAriance tables for models, letting you 
see if your simplified model is significantly different from the fuller model:
  anova(model1, model2)
## Analysis of Variance Table
##
## Model 1: Rate ~ Year + Age.Group + Ethnicity + Gender
## Model 2: Rate ~ Age.Group + Ethnicity + Gender
##   Res.Df      RSS Df Sum of Sq    F Pr(>F)
## 1    582 38243062
## 2    583 38252272 -1     -9210 0.14   0.71
The p-value in the righthand column is 0.71, so removing the year term didn’t 
significantly affect the model’s fit to the data.
The Akaike and Bayesian information criteria provide alternate methods of 
comparing models, via the AIC and BIC functions. They use log-likelihood values, 
which tell you how well the models fit the data, and penalize them depending upon 
how many terms there are in the model (so simpler models are better than
                                       complex models). 
Smaller numbers roughly correspond to “better” models:
  AIC(model1, model2)
##        df  AIC
## model1 19 8378
## model2 18 8376
BIC(model1, model2)
##        df  BIC
## model1 19 8462
## model2 18 8456
You can see the effects of these functions better if we create a silly model. 
Let’s remove age group, which appears to be a powerful predictor of gonorrhoea 
infection rates (as it should be, since children and the elderly have much 
                 less sex than young adults):
  silly_model <- update(model1, ~ . - Age.Group)
anova(model1, silly_model)

## Analysis of Variance Table
##
## Model 1: Rate ~ Year + Age.Group + Ethnicity + Gender
## Model 2: Rate ~ Year + Ethnicity + Gender
##   Res.Df      RSS  Df Sum of Sq    F Pr(>F)
## 1    582 38243062
## 2    593 57212506 -11  -1.9e+07 26.2 <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
AIC(model1, silly_model)
##             df  AIC
## model1      19 8378
## silly_model  8 8598
BIC(model1, silly_model)
##             df  BIC
## model1      19 8462
## silly_model  8 8633
In the silly model, anova notes a significant difference between the models, 
and both the AIC and the BIC have gone up by a lot.

